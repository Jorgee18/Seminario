{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f3b555a-ce71-42db-9a2c-57ffc77e6f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e0a9cfc-1073-41d2-a866-04c0ebbd0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Funciones\n",
    "\n",
    "# Normaliza a [0,1] por batch para que entrop√≠a y sensibilidad\n",
    "# Par√°metros:\n",
    "# v -> Es un tensor.\n",
    "# eps -> Es un valor de seguridad para evitar que la resta (v m√°ximo - v m√≠nimo) no sea cero.\n",
    "# La idea es normalizar v utilizando una funci√≥n para ello (v iesimo - v minimo / v maximo - v minimo).\n",
    "@torch.no_grad()\n",
    "def _normalize_batch(v, eps=1e-8):\n",
    "    vmin = v.min()\n",
    "    vmax = v.max()\n",
    "    return (v - vmin) / (vmax - vmin + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca9c2698-2d28-4824-a94c-6bd730eed250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title P√©rdida D-TRADES\n",
    "def d_trades_loss(\n",
    "    model,\n",
    "    x_natural,\n",
    "    y,\n",
    "    optimizer,\n",
    "    step_size=0.003,\n",
    "    epsilon=0.031,\n",
    "    perturb_steps=10,\n",
    "    distance='l_inf',\n",
    "    alpha=1.0,      # Peso de entrop√≠a\n",
    "    beta=1.0,       # Peso de sensibilidad\n",
    "    normalize_terms=True, # True: Para normalizar los valores de [B]; False: para usar los valores tal y como son\n",
    "    per_sample_sensitivity=True,  # True: Exacto por-ejemplo (loop); False: r√°pido por-batch (aprox.)\n",
    "    EPS=1e-12, # N√∫mero peque√±o para asegurar estabilidad en el calculo de la entrop√≠a\n",
    "):\n",
    "\n",
    "    # ---------- Ataque PGD (igual que TRADES) ----------\n",
    "    criterion_kl_sum = nn.KLDivLoss(reduction='sum')\n",
    "    model.eval()\n",
    "    batch_size = x_natural.size(0)\n",
    "\n",
    "    x_adv = x_natural.detach() + 0.001 * torch.randn_like(x_natural).detach()\n",
    "\n",
    "    if distance == 'l_inf':\n",
    "        for _ in range(perturb_steps):\n",
    "            x_adv.requires_grad_(True)\n",
    "            with torch.enable_grad():\n",
    "                loss_kl = criterion_kl_sum(\n",
    "                    F.log_softmax(model(x_adv), dim=1),\n",
    "                    F.softmax(model(x_natural), dim=1)\n",
    "                )\n",
    "            grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n",
    "            x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n",
    "            x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n",
    "            x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
    "    elif distance == 'l_2':\n",
    "        delta = 0.001 * torch.randn_like(x_natural).detach()\n",
    "        delta = Variable(delta.data, requires_grad=True)\n",
    "        opt_delta = torch.optim.SGD([delta], lr=epsilon / perturb_steps * 2)\n",
    "\n",
    "        for _ in range(perturb_steps):\n",
    "            adv = x_natural + delta\n",
    "            opt_delta.zero_grad()\n",
    "            with torch.enable_grad():\n",
    "                loss = -criterion_kl_sum(\n",
    "                    F.log_softmax(model(adv), dim=1),\n",
    "                    F.softmax(model(x_natural), dim=1)\n",
    "                )\n",
    "            loss.backward()\n",
    "            # renorm grad\n",
    "            grad_norms = delta.grad.view(batch_size, -1).norm(p=2, dim=1)\n",
    "            safe = grad_norms.clone()\n",
    "            safe[safe == 0] = 1.0\n",
    "            delta.grad.div_(safe.view(-1,1,1,1))\n",
    "            zero_mask = (grad_norms == 0).view(-1,1,1,1)\n",
    "            delta.grad[zero_mask] = torch.randn_like(delta.grad[zero_mask])\n",
    "            delta.grad.div_(grad_norms.view(-1,1,1,1))\n",
    "            opt_delta.step()\n",
    "            # proyecci√≥n\n",
    "            delta.data.add_(x_natural)\n",
    "            delta.data.clamp_(0,1).sub_(x_natural)\n",
    "            delta.data.renorm_(p=2, dim=0, maxnorm=epsilon)\n",
    "\n",
    "        x_adv = Variable(x_natural + delta, requires_grad=False)\n",
    "    else:\n",
    "        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # ---------- Forward limpio y p√©rdidas base ----------\n",
    "\n",
    "    # Salida del modelo para las imagenes limpias\n",
    "    logits_nat = model(x_natural)\n",
    "\n",
    "    # Salida de los ejemplos adversariales\n",
    "    logits_adv = model(x_adv.detach())\n",
    "\n",
    "    # Convierte los logits en probabilidades usando softmax\n",
    "    probs_nat = F.softmax(logits_nat, dim=1)\n",
    "\n",
    "    # Toma el algoritmo de las probabilidades adversariales\n",
    "    log_probs_adv = F.log_softmax(logits_adv, dim=1)\n",
    "\n",
    "    # P√©rdida de entrop√≠a cruzada est√°ndar para los datos limpios\n",
    "    loss_natural = F.cross_entropy(logits_nat, y, reduction='mean')\n",
    "\n",
    "    # Calcula la divergencia KL entre dos distribuciones\n",
    "    # Esto es para el calculo de lambda\n",
    "    # [B]\n",
    "    kl_per_example = F.kl_div(\n",
    "        log_probs_adv, probs_nat, reduction='none'\n",
    "    ).sum(dim=1)\n",
    "\n",
    "    # ---------- C√°lculo de entrop√≠a ----------\n",
    "    # probs_nat -> probabilidad de cada clase\n",
    "    # La f√≥rmula de la entrop√≠a es: - probabilidad de cada clase * logaritmo de la probabilidad.\n",
    "    # El sum(dim=1) es la suma sobre las clases, para cada fila del batch.\n",
    "    # Para evitar un inf dentro del logaritmo, si la probabilidad es cero, dentro del logaritmo.\n",
    "    # Se usa EPS, un valor muy bajo que reemplazar√° la probabilidad 0.\n",
    "    # Con el objetivo de evitar obtener un inf al momento de hacer log(0).\n",
    "    entropy = -(probs_nat * torch.log(probs_nat.clamp_min(EPS))).sum(dim=1)\n",
    "\n",
    "    # ---------- C√°lculo de sensibilidad ----------\n",
    "    # Calcula la norma de la gradiente de la KL (calculado anteriormente) respecto a la muestra adversarial\n",
    "    if per_sample_sensitivity:\n",
    "        # versi√≥n exacta por-ejemplo (loop) -- m√°s lenta, pero correcta\n",
    "        # Se toma una muestra x iesima con gradiente activo (requires_grad_(True)).\n",
    "        # Se calcula su KL individual con reduction='sum'.\n",
    "        # torch.autograd.grad devuelve la derivada parcial de L en x' (‚àÇKL/‚àÇx‚Ä≤).\n",
    "        # Se aplana y calcula su norma L2 (norm(p=2, dim=1)).\n",
    "        # Se acumulan todos los valores del kl_per_example llamada [B].\n",
    "\n",
    "        sens_list = []\n",
    "        model.eval()\n",
    "        for i in range(batch_size):\n",
    "            xi = x_adv[i:i+1].detach().clone().requires_grad_(True)\n",
    "            with torch.enable_grad():\n",
    "                kl_i = F.kl_div(\n",
    "                    F.log_softmax(model(xi), dim=1),\n",
    "                    probs_nat[i:i+1].detach(),\n",
    "                    reduction='sum'\n",
    "                )\n",
    "            gi = torch.autograd.grad(kl_i, xi, create_graph=False, retain_graph=False)[0]\n",
    "            sens_list.append(gi.view(gi.size(0), -1).norm(p=2, dim=1))\n",
    "        sensitivity = torch.cat(sens_list, dim=0)\n",
    "    else:\n",
    "        # versi√≥n por-batch (aprox): un √∫nico grad para la KL total\n",
    "        # kl_total: una KL promedio (escala batchmean).\n",
    "        # autograd.grad: gradiente ‚àÇKL_total/‚àÇx_adv.\n",
    "        # view(...).norm(p=2,dim=1): norma L2 obteniendo [B].\n",
    "\n",
    "        x_adv_req = x_adv.detach().clone().requires_grad_(True)\n",
    "        kl_total = F.kl_div(\n",
    "            F.log_softmax(model(x_adv_req), dim=1),\n",
    "            probs_nat.detach(),\n",
    "            reduction='batchmean'\n",
    "        )\n",
    "        g = torch.autograd.grad(kl_total, x_adv_req, create_graph=False)[0]\n",
    "        sensitivity = g.view(batch_size, -1).norm(p=2, dim=1)\n",
    "\n",
    "    # ---------- Construcci√≥n de lambda ----------\n",
    "    # Normaliza ambos vectores [B] al rango [0,1] si es que el flag es verdadero\n",
    "    # Combina ambos t√©rminos: ùúÜ i√©simo = ùõº * entrop√≠a + ùõΩ sensibilidad\n",
    "    #   alpha: peso de la entrop√≠a (controla la influencia de la incertidumbre).\n",
    "    #   beta: peso de la sensibilidad (controla la influencia de la vulnerabilidad adversarial).\n",
    "    # lam.detach(): Hace que Œª(x) se calcula fuera del gr√°fico de entrenamiento. Si no se detacha,\n",
    "    # PyTorch intentar√≠a retropropagar a trav√©s de las operaciones que generaron Œª(x),\n",
    "    # lo que distorsionar√≠a la p√©rdida.\n",
    "\n",
    "    if normalize_terms:\n",
    "        entropy_n = _normalize_batch(entropy)\n",
    "        sensitivity_n = _normalize_batch(sensitivity)\n",
    "    else:\n",
    "        entropy_n = entropy\n",
    "        sensitivity_n = sensitivity\n",
    "\n",
    "    lam = alpha * entropy_n + beta * sensitivity_n\n",
    "    lam = lam.detach()\n",
    "\n",
    "    # ---------- P√©rdida robusta ponderada din√°micamente ----------\n",
    "    # La p√©rdida adversarial dinamica que es la multiplicaci√≥n de:\n",
    "    # L_RD = El promedio de la sumatoria de (lambda dinamico * p√©rdida robusta por muestra)\n",
    "    loss_robust_dynamic = (lam * kl_per_example).mean()\n",
    "\n",
    "    # La p√©rdida total ahora es tal y como se hace en TRADES\n",
    "    loss_total = loss_natural + loss_robust_dynamic\n",
    "\n",
    "    # Retorna la p√©rdida total\n",
    "    return loss_total, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4fdc791-9b67-4ea7-8950-4e8db4fd6b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Dtrades.ipynb to script\n",
      "[NbConvertApp] Writing 8005 bytes to Dtrades.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Dtrades.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b55991-be6a-443e-a789-18627240d404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
