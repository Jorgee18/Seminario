{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aee93e0f-76fd-4c6c-becf-ae3ab9d64897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import torch, gc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from models.resnet import *\n",
    "from models.vgg import *\n",
    "from strategies.Dtrades import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72ab71e8-4641-4b73-b1d1-0ffdb35e6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch MNIST D-TRADES Adversarial Training')\n",
    "parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=128, metavar='N',\n",
    "                    help='input batch size for testing (default: 128)')\n",
    "parser.add_argument('--epochs', type=int, default=50, metavar='N',#default=100\n",
    "                    help='number of epochs to train')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                    help='SGD momentum')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--epsilon', default=0.3,\n",
    "                    help='perturbation')\n",
    "parser.add_argument('--num-steps', default=20,\n",
    "                    help='perturb number of steps')\n",
    "parser.add_argument('--step-size', default=0.01,\n",
    "                    help='perturb step size')\n",
    "parser.add_argument('--alpha', default=1,\n",
    "                    help='regularization, i.e., 1/lambda in D-TRADES')\n",
    "parser.add_argument('--beta', default=1,\n",
    "                    help='regularization, i.e., 1/lambda in D-TRADES')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--model-dir', default='./testing',\n",
    "                    help='directory of model for saving checkpoint')\n",
    "parser.add_argument('--save-freq', '-s', default=1, type=int, metavar='N',\n",
    "                    help='save frequency')\n",
    "parser.add_argument('--model', type=str, default='resnet', choices=['resnet', 'vgg'],\n",
    "                    help='Modelo a usar: resnet o vgg')\n",
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1adf9d33-39ed-488f-8b30-33b8ad56ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustes\n",
    "model_dir = args.model_dir\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "806ac204-6b7c-433a-a0c6-17b8cc8b3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar el cargador de datos\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False,\n",
    "                   transform=transforms.ToTensor()),\n",
    "                   batch_size=args.test_batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "628ae062-7c37-4daa-973b-4ac7bef3b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_min = []\n",
    "lambda_max = []\n",
    "lambda_mean = []\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # calculate robust loss\n",
    "        loss, lambda_value = d_trades_loss(model=model,\n",
    "                           x_natural=data,\n",
    "                           y=target,\n",
    "                           optimizer=optimizer,\n",
    "                           step_size=args.step_size,\n",
    "                           epsilon=args.epsilon,\n",
    "                           perturb_steps=args.num_steps,\n",
    "                           alpha=args.alpha,\n",
    "                           beta=args.beta,\n",
    "                        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print progress\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            lambda_min.append(lambda_value.min().item())\n",
    "            lambda_max.append(lambda_value.max().item())\n",
    "            lambda_mean.append(lambda_value.mean().item())\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tLambda Mean: {:.2f} \\tLambda Min: {:.2f} \\tLambda Max: {:.2f}'.format(\n",
    "                                            epoch,\n",
    "                                            batch_idx * len(data),\n",
    "                                            len(train_loader.dataset),\n",
    "                                            100. * batch_idx / len(train_loader),\n",
    "                                            loss.item(),\n",
    "                                            lambda_value.mean().item(),\n",
    "                                            lambda_value.min().item(),\n",
    "                                            lambda_value.max().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64108166-f237-4915-b56f-4a2fcb9f54ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pgd_whitebox(model,\n",
    "                  X,\n",
    "                  y,\n",
    "                  epsilon=args.epsilon,\n",
    "                  num_steps=20,\n",
    "                  step_size=0.003):\n",
    "    out = model(X)\n",
    "    err = (out.data.max(1)[1] != y.data).float().sum()\n",
    "    X_pgd = Variable(X.data, requires_grad=True)\n",
    "\n",
    "    random_noise = torch.FloatTensor(*X_pgd.shape).uniform_(-epsilon, epsilon).to(device)\n",
    "    X_pgd = Variable(X_pgd.data + random_noise, requires_grad=True)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        opt = optim.SGD([X_pgd], lr=1e-3)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            loss = nn.CrossEntropyLoss()(model(X_pgd), y)\n",
    "        loss.backward()\n",
    "        eta = step_size * X_pgd.grad.data.sign()\n",
    "        X_pgd = Variable(X_pgd.data + eta, requires_grad=True)\n",
    "        eta = torch.clamp(X_pgd.data - X.data, -epsilon, epsilon)\n",
    "        X_pgd = Variable(X.data + eta, requires_grad=True)\n",
    "        X_pgd = Variable(torch.clamp(X_pgd, 0, 1.0), requires_grad=True)\n",
    "    err_pgd = (model(X_pgd).data.max(1)[1] != y.data).float().sum()\n",
    "    return err, err_pgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6021d23-b8fa-464d-95d7-1d19d750c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_adv_test_whitebox(model, device, test_loader):\n",
    "\n",
    "    model.eval()\n",
    "    robust_err_total = 0\n",
    "    natural_err_total = 0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # pgd attack\n",
    "        X, y = Variable(data, requires_grad=True), Variable(target)\n",
    "        err_natural, err_robust = _pgd_whitebox(model, X, y)\n",
    "        robust_err_total += err_robust\n",
    "        natural_err_total += err_natural\n",
    "        \n",
    "    natural_acc = 1 - natural_err_total / len(test_loader.dataset)\n",
    "    robust_acc = 1- robust_err_total / len(test_loader.dataset)\n",
    "    robust_drop = natural_acc - robust_acc\n",
    "    attack_success_rate = 1 - robust_acc\n",
    "    \n",
    "    print(f'PGD natural_acc: {natural_acc:.4f}, robust_acc: {robust_acc:.4f}, robust_drop: {robust_drop:4f}, attack_success_rate: {attack_success_rate:4f}')\n",
    "    return natural_acc, robust_acc, robust_drop, attack_success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8fd9e6b-9d57-4707-ac90-614f929f420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"decrease the learning rate\"\"\"\n",
    "    lr = args.lr\n",
    "    if epoch >= 55:\n",
    "        lr = args.lr * 0.1\n",
    "    if epoch >= 75:\n",
    "        lr = args.lr * 0.01\n",
    "    if epoch >= 90:\n",
    "        lr = args.lr * 0.001\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b5396aa-be50-4358-9a84-ba3de1c1995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75e7243e-bd61-4cb7-a871-30a4e3072eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.401673 \tLambda Mean: 1.05 \tLambda Min: 0.81 \tLambda Max: 1.22\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.266848 \tLambda Mean: 0.34 \tLambda Min: 0.00 \tLambda Max: 1.47\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.221973 \tLambda Mean: 0.31 \tLambda Min: 0.00 \tLambda Max: 1.41\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Max Robust Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(hist[\u001b[33m'\u001b[39m\u001b[33mrobust_acc\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     21\u001b[39m start_time = time.time()\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Entrenamiento adversarial\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m================================================================\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Evaluación\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(args, model, device, train_loader, optimizer, epoch)\u001b[39m\n\u001b[32m      9\u001b[39m optimizer.zero_grad()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# calculate robust loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m loss, lambda_value = \u001b[43md_trades_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mx_natural\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m                   \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m                   \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mperturb_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m                   \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m loss.backward()\n\u001b[32m     23\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Universidad/Seminario/workspace/strategies/Dtrades.py:153\u001b[39m, in \u001b[36md_trades_loss\u001b[39m\u001b[34m(model, x_natural, y, optimizer, step_size, epsilon, perturb_steps, distance, alpha, beta, normalize_terms, per_sample_sensitivity, EPS)\u001b[39m\n\u001b[32m    150\u001b[39m xi = x_adv[i:i+\u001b[32m1\u001b[39m].detach().clone().requires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m    152\u001b[39m     kl_i = F.kl_div(\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m         F.log_softmax(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m)\u001b[49m, dim=\u001b[32m1\u001b[39m),\n\u001b[32m    154\u001b[39m         probs_nat[i:i+\u001b[32m1\u001b[39m].detach(),\n\u001b[32m    155\u001b[39m         reduction=\u001b[33m'\u001b[39m\u001b[33msum\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    156\u001b[39m     )\n\u001b[32m    157\u001b[39m gi = torch.autograd.grad(kl_i, xi, create_graph=\u001b[38;5;28;01mFalse\u001b[39;00m, retain_graph=\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m    158\u001b[39m sens_list.append(gi.view(gi.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m).norm(p=\u001b[32m2\u001b[39m, dim=\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Universidad/Seminario/workspace/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Universidad/Seminario/workspace/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Universidad/Seminario/workspace/models/vgg.py:35\u001b[39m, in \u001b[36mVGG.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     out = out.view(out.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m     37\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.classifier(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Universidad/Seminario/workspace/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Universidad/Seminario/workspace/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Universidad/Seminario/workspace/venv/lib/python3.13/site-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Universidad/Seminario/workspace/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Universidad/Seminario/workspace/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Universidad/Seminario/workspace/venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Universidad/Seminario/workspace/venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    if args.model.lower() == \"resnet\":\n",
    "        model = ResNet18()\n",
    "        model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    elif args.model.lower() == \"vgg\":\n",
    "        model = vgg16(in_channels=1)\n",
    "    else:\n",
    "        raise ValueError(\"Modelo no reconocido: usa --model resnet o --model vgg\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "    # Historial\n",
    "    history = {'natural_acc': [], 'robust_acc': [], 'robust_drop': [], 'attack_succes_rate': []}\n",
    "    results = {}\n",
    "    \n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        # Entrenamiento adversarial\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "\n",
    "        print('================================================================')\n",
    "        \n",
    "        # Evaluación\n",
    "        natural_acc, robust_acc, robust_drop, attack_succes_rate = eval_adv_test_whitebox(\n",
    "            model, device, test_loader\n",
    "        )\n",
    "\n",
    "        print('using time:', time.time() - start_time)\n",
    "        print('================================================================')\n",
    "        \n",
    "        # Guardar historial\n",
    "        history['natural_acc'].append(natural_acc)\n",
    "        history['robust_acc'].append(robust_acc)\n",
    "        history['robust_drop'].append(robust_drop)\n",
    "        history['attack_succes_rate'].append(attack_succes_rate)\n",
    "\n",
    "        results[f'history{epoch}'] = history.copy()\n",
    "\n",
    "        # Guardar checkpoints\n",
    "        if epoch % args.save_freq == 0:\n",
    "            torch.save(model.state_dict(),\n",
    "                       os.path.join(model_dir, f'model-nn-epoch{epoch}.pt'))\n",
    "            torch.save(optimizer.state_dict(),\n",
    "                       os.path.join(model_dir, f'opt-nn-checkpoint_epoch{epoch}.tar'))\n",
    "    \n",
    "    print('================================================================')\n",
    "    print(\"\\nResumen de Resultados por historial:\")\n",
    "    for ep, hist in results.items():\n",
    "        print(f\"{ep}: Max Robust Acc: {max(hist['robust_acc']):.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
